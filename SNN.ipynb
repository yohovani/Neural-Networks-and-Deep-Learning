{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af8a97b6",
   "metadata": {},
   "source": [
    "# SNN\n",
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b334341d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "#Extracting first 100 samples petaining to iris setosa and verginica\n",
    "X = iris.data[:100, :4]\n",
    "#actual output\n",
    "Y = iris.target[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bbbae95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 100)\n",
      "(1, 100)\n"
     ]
    }
   ],
   "source": [
    "#Getting Dimensions Right\n",
    "\n",
    "#Now the data X_norm is of shape (100,4)\n",
    "\n",
    "#But for SNN we need to have the data of shape (no_of_features x no_of_samples). So take a transpose of X_norm.\n",
    "\n",
    "#The Y data is a list of shape (100,). Convert it a vector of shape (1,100) by using reshape() function.\n",
    "\n",
    "X_norm = X.reshape(100,4)\n",
    "X_data = X_norm.T\n",
    "Y_data = Y.reshape(1,100)\n",
    "\n",
    "print(X_data.shape)\n",
    "print(Y_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10c53d7",
   "metadata": {},
   "source": [
    "## Initialising Weights and Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a90b542",
   "metadata": {},
   "source": [
    "<li>Before we start the forward propagation, we need to initialize weights and bias to some random values.</li>\n",
    "\n",
    "<li>Since we have four features, we need to have weight vector of shape (4,1) and one bias term of shape (1,1).</li>\n",
    "\n",
    "<li>In this case, we initialize all our weights and bias to zero.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c4e5938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialiseNetwork(num_features):\n",
    "    W = np.zeros((num_features, 1))\n",
    "    b = 0\n",
    "    parameters = {\"W\": W, \"b\": b}\n",
    "\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89ff9fa",
   "metadata": {},
   "source": [
    "## Defining Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b971bc",
   "metadata": {},
   "source": [
    "<li>Before going with the forward propagation, we need to define an activation function for the neuron.</li>\n",
    "\n",
    "<li>Since this is a binary classification, let's consider a sigmoid function that maps any linear input to values between 0 to 1.</li>\n",
    "\n",
    "<li>The sigmoid activation function is implemented as shown in the below code snippet.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "996227fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2535db0",
   "metadata": {},
   "source": [
    "## Forward Propagation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d836a02",
   "metadata": {},
   "source": [
    "<li>You have seen the theorical implementation of forward propagation in the previous topic </li><br>\n",
    "<li>The same is implemented in Python as follows</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8322f28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardPropagation(X, Y, parameters):\n",
    "    W = parameters[\"W\"]\n",
    "    b = parameters[\"b\"]\n",
    "    Z = np.dot(W.T,X) + b\n",
    "    A = sigmoid(Z)\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a857784",
   "metadata": {},
   "source": [
    "- Calculating cost function for a given number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa92d231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(A, Y, num_samples):\n",
    "    return -1/num_samples *np.sum(Y*np.log(A) + (1-Y)*(np.log(1-A)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3ff33c",
   "metadata": {},
   "source": [
    "## Defining Backpropagation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbfbcf9",
   "metadata": {},
   "source": [
    "-From forward propagation, you know the output A <br> - Using this output, you need to find the derivatives of weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05c2bf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backPropagation(X, Y, A, num_samples):\n",
    "    dZ = A - Y\n",
    "    dW = (np.dot(X, dZ.T))/num_samples\n",
    "    db = np.sum(dZ)/num_samples\n",
    "    \n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b04d4f7",
   "metadata": {},
   "source": [
    "## Updating Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15d7728",
   "metadata": {},
   "source": [
    "<li> Once we have the derivatives, you need to substract them from original weughts and bias </li> <br> <li> While subtracting, we multiply the derivatives with learning rate to have control over the gradient at each step of iteration </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab5c3006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateParameters(parameters, dW, db, learning_rate):\n",
    "    W = parameters[\"W\"] - (learning_rate * dW)\n",
    "    b = parameters[\"b\"] - (learning_rate * db)\n",
    "    \n",
    "    return {\"W\":W, \"b\":b}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707e35ea",
   "metadata": {},
   "source": [
    "## Defining the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5df2bc",
   "metadata": {},
   "source": [
    "<li> Using all the function defined so far let's define the model to intilize and train the SNN</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6b7795b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, num_iter, learning_rate):\n",
    "    num_features = X.shape[0]\n",
    "    num_samples = float(X.shape[1])\n",
    "    parameters = initialiseNetwork(num_features)\n",
    "    for i in range(num_iter):\n",
    "        A = forwardPropagation(X, Y, parameters)\n",
    "        if(i%100 == 0):\n",
    "            print(\"cost after {} iteration: {}\".format(i, cost(A, Y, num_samples)))\n",
    "        dW, db = backPropagation(X, Y, A, num_samples)\n",
    "        parameters = updateParameters(parameters, dW, db, learning_rate)\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112123ba",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e186ac",
   "metadata": {},
   "source": [
    "<li> Train the model using iris dataset with learning rate 0.1 and number of iteration equal to 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "735bbf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost after 0 iteration: 0.6931471805599453\n",
      "cost after 100 iteration: 0.06656095976383733\n",
      "cost after 200 iteration: 0.03492693805012102\n",
      "cost after 300 iteration: 0.02393176289715331\n",
      "cost after 400 iteration: 0.018316011387467374\n",
      "cost after 500 iteration: 0.014894365777233843\n",
      "cost after 600 iteration: 0.012584644199696762\n",
      "cost after 700 iteration: 0.010917147703633785\n",
      "cost after 800 iteration: 0.0096547082608724\n",
      "cost after 900 iteration: 0.008664477279333902\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "parameters = model(X_data, Y, 1000, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604ad9c4",
   "metadata": {},
   "source": [
    "<li> You can see that at every iteration the const is reducing approaching close to zero </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7339eed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
